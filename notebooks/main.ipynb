{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project notebook\n",
    "\n",
    "The following notebook is an excerpt and re-written example from a _real_ production model.\n",
    "\n",
    "The overall purpose of the ML algorithm is to identify users on the website that are new possible customers. This is done by collecting behaviour data from the users as input, and the target is whether they converted/turned into customers -- essentially a classification problem. \n",
    "\n",
    "This notebook only focuses on the data processing part. As you know, there are multiple steps in an ML pipeline, and it's not always they are neatly separated like this. For the exam project, they will not be, and that is part of the challenge for you. For production code, it should also not be Python notebooks since, as you may well see, it is difficult to work with and collaborate on them in an automated way.\n",
    "\n",
    "There is a lot of \"fluff\" in such a notebook. This ranges from comments and markdown cells to commented out code and random print statements. That is not necessary in a properly managed project where you can use git to check the version history and such. \n",
    "\n",
    "What is important for you is the identify the entry points into the code and segment them out into easily understandable chunks. Additionally, you might want to follow some basic code standards, such as:\n",
    "\n",
    "- Import only libraries in the beginning of the files\n",
    "- Define functions in the top of the scripts, or if used multiple places, move into a util.py script or such\n",
    "- Remove unused/commented out code\n",
    "- Follow the [PEP 8](https://peps.python.org/pep-0008/) style guide (and others)\n",
    "  \n",
    "Another thing to note is that comments can be misleading. Even if the markdown cell or inline comments says it does _X_, don't be surprised if it actually does _Y_. Sometimes additional text can be a blessing, but it can also be a curse sometimes. Remember, though, that your task is to make sure the code runs as before after refactoring the notebook into other files, not update/improve the model or flow to reflect what the comments might say.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PROCESSING\n",
    "\n",
    "In this section, we will perform Exploratory Data Analysis (EDA) to better understand the dataset before proceeding with more advanced analysis. EDA helps us get a sense of the data’s structure, identify patterns, and spot any potential issues like missing values or outliers. By doing so, we can gain a clearer understanding of the data's key characteristics.\n",
    "\n",
    "We will start with summary statistics to review basic measures like mean, median, and variance, providing an initial overview of the data distribution. Then, we’ll create visualizations such as histograms, box plots, and scatter plots to explore relationships between variables, check for any skewness, and highlight outliers.\n",
    "\n",
    "The purpose of this EDA is to ensure that the dataset is clean and well-structured for further analysis. This step also helps us identify any necessary data transformations and informs decisions on which features might be most relevant for modeling in later stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create artifact directory\n",
    "We want to create a directory for storing all the artifacts in the current directory. Users can load all the artifacts later for data cleaning pipelines and inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbutils.widgets.text(\"Training data max date\", \"2024-01-31\")\n",
    "# dbutils.widgets.text(\"Training data min date\", \"2024-01-01\")\n",
    "# max_date = dbutils.widgets.get(\"Training data max date\")\n",
    "# min_date = dbutils.widgets.get(\"Training data min date\")\n",
    "\n",
    "# testnng\n",
    "max_date = \"2024-01-31\"\n",
    "min_date = \"2024-01-01\"\n",
    "\n",
    "#C: hard-coded dates should eventually be moved into env. variables, config or otherwise.\n",
    "#C: commented-out code is safe to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #C: file-system utilities (path handling, directory creation etc.)\n",
    "import shutil #C: advanced file operations - check actually used in code, since it's matted out.\n",
    "from pprint import pprint #C: pretty-printing objects for debugging\n",
    "\n",
    "# shutil.rmtree(\"./artifacts\",ignore_errors=True) #C: deletes the \"artifacts\" folder.\n",
    "os.makedirs(\"artifacts\",exist_ok=True) #C: creates directory named \"artifacts\". exist_ok=True prevents errors if dir already exists.\n",
    "print(\"Created artifacts directory\") #C: print statement can be removed or replaced with logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas dataframe print options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') #C: silences all warnings across every library. Not recommended in production, used to keep notebook-output clean.\n",
    "pd.set_option('display.float_format',lambda x: \"%.3f\" % x) #C: sets global pd display format so floats are 3 decimals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "\n",
    "* **describe_numeric_col**: Calculates various descriptive stats for a numeric column in a dataframe.\n",
    "* **impute_missing_values**: Imputes the mean/median for numeric columns or the mode for other types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_numeric_col(x):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        x (pd.Series): Pandas col to describe.\n",
    "    Output:\n",
    "        y (pd.Series): Pandas series with descriptive stats. \n",
    "    \"\"\"\n",
    "    return pd.Series(\n",
    "        [x.count(), x.isnull().count(), x.mean(), x.min(), x.max()],\n",
    "        index=[\"Count\", \"Missing\", \"Mean\", \"Min\", \"Max\"]\n",
    "    )\n",
    "\n",
    "#C: ^function creates a new pandas series containing\n",
    "# the number of non-null values\n",
    "# number of null values\n",
    "# mean, min & max of the column\n",
    "# and labels those values with index names\n",
    "# assumes x is numeric\n",
    "# might just be used for exploratory data analysis and will not be needed in pipeline possibly.\n",
    "\n",
    "def impute_missing_values(x, method=\"mean\"):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        x (pd.Series): Pandas col to describe.\n",
    "        method (str): Values: \"mean\", \"median\"\n",
    "    \"\"\"\n",
    "    if (x.dtype == \"float64\") | (x.dtype == \"int64\"): #C: checks if column is numeric\n",
    "        x = x.fillna(x.mean()) if method==\"mean\" else x.fillna(x.median()) #C: fills missing values with column mean if method input is set to mean, else fills with median.\n",
    "    else:\n",
    "        x = x.fillna(x.mode()[0]) #C: if datatype is not float or int, fills the column with mode (most frequent value)\n",
    "    return x #C: returns the filled-out column.\n",
    "\n",
    "#C: ^function takes a pandas Series and replaces missing values with column mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data\n",
    "\n",
    "We read the latest data from our data lake source. Here we load it locally after having pulled it from DVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc pull #C: ! tells notebook to run a shell command, not Python code.\n",
    "#C: looks at dvc-tracked data files, downloads the actual data from remote storage configured in repo,\n",
    "# and ensures local wokspace has the exact dataset version the pipeline expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading training data\") #C: remove print statement\n",
    "\n",
    "data = pd.read_csv(\"./artifacts/raw_data.csv\") #C: reads data into pd dataframe\n",
    "\n",
    "print(\"Total rows:\", data.count()) #C: print statement, remove in future\n",
    "display(data.head(5)) #C: display first five rows, remove in future\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #C: imports pandas again, not needed unless part of a separate .py file\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "if not max_date: #C: checks if max date is empty/None/False\n",
    "    max_date = pd.to_datetime(datetime.datetime.now().date()).date() #C: sets max date to today's date and converts to datetime.date object\n",
    "else:\n",
    "    max_date = pd.to_datetime(max_date).date() #C: if max_date is already set, converts it to a datetime.date object\n",
    "\n",
    "min_date = pd.to_datetime(min_date).date() #C: converts hard-coded min date from earlier to datetime.date object\n",
    "\n",
    "# Time limit data\n",
    "data[\"date_part\"] = pd.to_datetime(data[\"date_part\"]).dt.date #C: converts the column \"date_part\" in the data to datetime and then extracts the date\n",
    "data = data[(data[\"date_part\"] >= min_date) & (data[\"date_part\"] <= max_date)] #C: Filters rows so that date_part is between min_date and max_date, inclusive.\n",
    "\n",
    "min_date = data[\"date_part\"].min() #C: finds actual min date in filtered data\n",
    "max_date = data[\"date_part\"].max() #C: same as above but max date\n",
    "date_limits = {\"min_date\": str(min_date), \"max_date\": str(max_date)} #C: creates dict with min_date and max_date as strings\n",
    "with open(\"./artifacts/date_limits.json\", \"w\") as f:\n",
    "    json.dump(date_limits, f) #C: saves the dictionary as a .json file so downstream stages knows the data boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "Not all columns are relevant for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\n",
    "    [\n",
    "        \"is_active\", \"marketing_consent\", \"first_booking\", \"existing_customer\", \"last_seen\"\n",
    "    ],\n",
    "    axis=1 #C: removes columns, not rows\n",
    ")\n",
    "#C: removes columns from dataframe that are irrelevant to the model or to preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing columns that will be added back after the EDA - C: correct.\n",
    "data = data.drop(\n",
    "    [\"domain\", \"country\", \"visited_learn_more_before_booking\", \"visited_faq\"],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "* Remove rows with empty target variable\n",
    "* Remove rows with other invalid column data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data[\"lead_indicator\"].replace(\"\", np.nan, inplace=True)\n",
    "data[\"lead_id\"].replace(\"\", np.nan, inplace=True)\n",
    "data[\"customer_code\"].replace(\"\", np.nan, inplace=True)\n",
    "#C: ^ replaces empty strings in three columns with NaN. inplace=True modifies the dataframe directly.\n",
    "# this ensures missing values are correctly marked as NaN, which is required for .dropna etc.\n",
    "\n",
    "data = data.dropna(axis=0, subset=[\"lead_indicator\"])\n",
    "data = data.dropna(axis=0, subset=[\"lead_id\"])\n",
    "#C: ^ removes rows where lead_indicator or lead_id is NaN.\n",
    "\n",
    "data = data[data.source == \"signup\"] #C: filters dataset to only include rows where source columns equals \"signup\"\n",
    "result=data.lead_indicator.value_counts(normalize = True) #C: counts unique values in the lead_indicator column. normalize=True gives fractions instead of counts.\n",
    "\n",
    "print(\"Target value counter\")\n",
    "for val, n in zip(result.index, result):\n",
    "    print(val, \": \", n)\n",
    "data\n",
    "#C: ^ print statement and display of data, remove in future or replace with logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create categorical data columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = [\n",
    "    \"lead_id\", \"lead_indicator\", \"customer_group\", \"onboarding\", \"source\", \"customer_code\"\n",
    "] #C: define a lit of column names that will be converted. Categorical/identifier columns\n",
    "\n",
    "for col in vars:\n",
    "    data[col] = data[col].astype(\"object\") #C: converts datatype of each column to object.\n",
    "    print(f\"Changed {col} to object type\") #C: print statement, remove later or replace with logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate categorical and continuous columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_vars = data.loc[:, ((data.dtypes==\"float64\")|(data.dtypes==\"int64\"))] #C: select continuous (numeric) columns and save to variable cont_vars\n",
    "cat_vars = data.loc[:, (data.dtypes==\"object\")] #C: same as above but for categorical columns, saved to cat_vars\n",
    "\n",
    "print(\"\\nContinuous columns: \\n\")\n",
    "pprint(list(cont_vars.columns), indent=4) #C: converts cont. column names to a list, uses pprint for nice formatting\n",
    "print(\"\\n Categorical columns: \\n\")\n",
    "pprint(list(cat_vars.columns), indent=4) #C: converts cat. column names to a list, uses pprint for nice formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers\n",
    "\n",
    "Outliers are data points that significantly differ from the majority of observations in a dataset and can distort statistical analysis or model performance. To identify and remove outliers, one common method is to use the Z-score, which measures how many standard deviations a data point is from the mean. Data points with a Z-score greater than 2 (or sometimes 3) standard deviations away from the mean are typically considered outliers. By applying this threshold, we can filter out values that fall outside the normal range of the data, ensuring that the remaining dataset is more representative and less influenced by extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_vars = cont_vars.apply(lambda x: x.clip(lower = (x.mean()-2*x.std()),\n",
    "                                             upper = (x.mean()+2*x.std()))) #C: restricts values to a given range (pm 2 std. deviations of mean)\n",
    "outlier_summary = cont_vars.apply(describe_numeric_col).T #C: summarizes each continuous column after removing outliers\n",
    "outlier_summary.to_csv('./artifacts/outlier_summary.csv') #C: writes the outlier summary to a csv in artifacts folder\n",
    "outlier_summary #C: displays summary, remove later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute data\n",
    "\n",
    "In real-world datasets, missing data is a common occurrence due to various factors such as human error, incomplete data collection processes, or system failures. These gaps in the data can hinder analysis and lead to biased results if not properly addressed. Since many analytical and machine learning algorithms require complete data, handling missing values is an essential step in the data preprocessing phase.\n",
    "\n",
    "In the next code block, we will handle missing data by performing imputation. For numerical columns, we will replace missing values with the mean or median of the entire column, which provides a reasonable estimate based on the existing data. For categorical columns (object type), we will use the mode, or most frequent value, to fill in missing entries. This approach helps us maintain a complete dataset while ensuring that the imputed values align with the general distribution of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_missing_impute = cat_vars.mode(numeric_only=False, dropna=True) #C: prepares a dictionary of replacement values for imputing missing categorical data later from the modes, ignoring missing data when computing these.\n",
    "cat_missing_impute.to_csv(\"./artifacts/cat_missing_impute.csv\") #C: saves modes to a csv file in artifacts folder\n",
    "cat_missing_impute #C: displays modes, remove later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous variables missing values\n",
    "cont_vars = cont_vars.apply(impute_missing_values) #C: fills missing values in numeric columns with mean or median (if specified) using helper function impute_missing_values()\n",
    "cont_vars.apply(describe_numeric_col).T #C: produces descriptive stats after imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars.loc[cat_vars['customer_code'].isna(),'customer_code'] = 'None' #C: selects rows in cat_vars where customer_code is NaN and replaces them with string \"None\". Ensures no missing values before categorical imputation (so not imputed)\n",
    "cat_vars = cat_vars.apply(impute_missing_values) #C: applies imputation function to cat columns. Replaces NaN with the mode\n",
    "cat_vars.apply(lambda x: pd.Series([x.count(), x.isnull().sum()], index = ['Count', 'Missing'])).T #C: creates table showing # of non-missing and missing values in each column. Verifies that missing values have been imputed.\n",
    "cat_vars # C: display, remove later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data standardisation\n",
    "\n",
    "Standardization, or scaling, becomes necessary when continuous independent variables are measured on different scales, as this can lead to unequal contributions to the analysis. The objective is to rescale these variables so they have comparable ranges and/or variances, ensuring a more balanced influence in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler #C: scales numeric features to a 0-1 range\n",
    "import joblib #C: used to serialize Python objects\n",
    "\n",
    "scaler_path = \"./artifacts/scaler.pkl\" #C: setting filepath for storage of scaler\n",
    "\n",
    "scaler = MinMaxScaler() #C: initializes a new MinMax scaler\n",
    "scaler.fit(cont_vars) #C: fits the scaler on continuous/numeric columns\n",
    "\n",
    "joblib.dump(value=scaler, filename=scaler_path) #C: saves the scaler to artifacts\n",
    "print(\"Saved scaler in artifacts\") #C: print statement, remove or replace with logging.\n",
    "\n",
    "cont_vars = pd.DataFrame(scaler.transform(cont_vars), columns=cont_vars.columns) #C: applies MinMax scaling to the numeric columns and converts the result into a df with original col. names\n",
    "cont_vars #C: display, remove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_vars = cont_vars.reset_index(drop=True) #C: resets continuous variables' row index after cleaning. drop=True ensures old index is not added as column.\n",
    "cat_vars = cat_vars.reset_index(drop=True) #C: same as above for categorical variables.\n",
    "data = pd.concat([cat_vars, cont_vars], axis=1) #C: concatenates the categorical and continuous dataframes column-wise.\n",
    "print(f\"Data cleansed and combined.\\nRows: {len(data)}\") #C: print-statement, remove or replace with logging.\n",
    "data #C: display, remove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data drift artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json #C: imports json again\n",
    "\n",
    "data_columns = list(data.columns) #C: creates a list from all columns in the dataframe.\n",
    "with open('./artifacts/columns_drift.json','w+') as f:           \n",
    "    json.dump(data_columns,f) #C: writes the list of column names to .json - to monitor column drift in production, to detect changes or missing features.\n",
    "    \n",
    "data.to_csv('./artifacts/training_data.csv', index=False) #C: writes the fully processed dataframe to the artifacts folder. index=False ensures no column containing indices. This is the final, training-ready dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning object columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns #C: display, remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['bin_source'] = data['source'] #C: creates a new column, \"bin_source\" - initially a copy of \"source\" column.\n",
    "values_list = ['li', 'organic','signup','fb'] #C: specifies which original \"source\" values are explicitly handled\n",
    "data.loc[~data['source'].isin(values_list),'bin_source'] = 'Others' #C: for rows where \"source\" is not in values_list, sets \"bin_source\" to \"Others\"\n",
    "mapping = {'li' : 'socials', \n",
    "           'fb' : 'socials', \n",
    "           'organic': 'group1', \n",
    "           'signup': 'group1'\n",
    "           } #C: maps specific source values to broader categories for modeling (either socials or group1)\n",
    "\n",
    "data['bin_source'] = data['source'].map(mapping) #replaces original \"bin_source\" values according to mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save gold medallion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(f\"drop table if exists train_gold\") - C: safe to remove.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_gold = spark.createDataFrame(data)\n",
    "# data_gold.write.saveAsTable('train_gold')\n",
    "# dbutils.notebook.exit(('training_golden_data',most_recent_date))\n",
    "\n",
    "#C: all commented-out lines safe to remove.\n",
    "\n",
    "data.to_csv('./artifacts/train_data_gold.csv', index=False) #C: saves the fully processed dataframe to a csv file in artifacts folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAINING\n",
    "\n",
    "Training the model uses a training dataset for training an ML algorithm. It has sample output data and the matching input data that affects the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Constants used:\n",
    "current_date = datetime.datetime.now().strftime(\"%Y_%B_%d\") #C: gets current date and time and formats it as yyyy_monthname_dd\n",
    "data_gold_path = \"./artifacts/train_data_gold.csv\" #C: points to the csv file saved earlier, path for training data\n",
    "data_version = \"00000\" #C: placeholder for data version ID\n",
    "experiment_name = current_date #C: uses the current date as the experiment name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create paths\n",
    "\n",
    "Maybe the artifacts path has not been created during data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True) #C: creates artifacts folder if one does not already exist\n",
    "os.makedirs(\"mlruns\", exist_ok=True) #C: creates a folder named \"mlruns\"\n",
    "os.makedirs(\"mlruns/.trash\", exist_ok=True) #C: creates a hidden folder, \".trash\" inside \"mlruns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow #C: a library for experiment tracking, model versioning and logging metrics/artifactss\n",
    "\n",
    "mlflow.set_experiment(experiment_name) #C: tells mlflow to use the current experiment. Is created if it does not already exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "\n",
    "* *create_dummies*: Create one-hot encoding columns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_cols(df, col): #C: defines function that takes a dataframe and a list of column to be one-hot encoded\n",
    "    df_dummies = pd.get_dummies(df[col], prefix=col, drop_first=True) #C: converts categorical values into separate 0/1 columns (one-hot encoding)\n",
    "    new_df = pd.concat([df, df_dummies], axis=1) #C: concatenates new dummy columns with original dataframe column-wise\n",
    "    new_df = new_df.drop(col, axis=1) #C: drops the original column since its information is now captured by dummy columns.\n",
    "    return new_df #C: returns new dataframe with dummy variables included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data\n",
    "We use the training data we cleaned earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_gold_path) #C: reads the ready training-data into a pandas dataframe \"data\"\n",
    "print(f\"Training data length: {len(data)}\") #C: prints the number of observations in dataframe. Can be removed or replaced with logging.\n",
    "data.head(5) #C: display, remove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data type split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"lead_id\", \"customer_code\", \"date_part\"], axis=1) #C: removes columns not used for modeling\n",
    "\n",
    "cat_cols = [\"customer_group\", \"onboarding\", \"bin_source\", \"source\"] #C: lists columns that are categorical features for modeling\n",
    "cat_vars = data[cat_cols] #C: extracts categorical features from the dataframe.\n",
    "\n",
    "other_vars = data.drop(cat_cols, axis=1) #C: saves non-categorical features in variable \"other_vars\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy variable for categorical vars\n",
    "\n",
    "1. Create one-hot encoded cols for cat vars\n",
    "2. Change to floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for col in cat_vars:\n",
    "    cat_vars[col] = cat_vars[col].astype(\"category\")\n",
    "    cat_vars = create_dummy_cols(cat_vars, col)\n",
    "#C: ^ loops over each categorical column and converts it to category datatype. One-hot encodes the column.\n",
    "\n",
    "data = pd.concat([other_vars, cat_vars], axis=1) #C: combines other_vars and one-hot encoded categorical features.\n",
    "\n",
    "for col in data:\n",
    "    data[col] = data[col].astype(\"float64\")\n",
    "    print(f\"Changed column {col} to float\")\n",
    "#C: ^ converts every column in data to datatype float. Categorical values are one-hot encoded, so all features are now numerical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"lead_indicator\"] #C: saves the label column to variable y\n",
    "X = data.drop([\"lead_indicator\"], axis=1) #C: saves the feature columns with no label column to variable X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42, test_size=0.15, stratify=y\n",
    ") #C: splits the training data into a training and a validation/evaluation test set.\n",
    "y_train #C: display, remove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "This stage involves training the ML algorithm by providing it with datasets, where the learning process takes place. Consistent training can significantly enhance the model's prediction accuracy. It's essential to initialize the model's weights randomly so the algorithm can effectively learn to adjust them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRFClassifier #C: the XGBoost random forest classifier\n",
    "from sklearn.model_selection import RandomizedSearchCV #C: tool for random hyperparameter search\n",
    "from scipy.stats import uniform #C: uniform distribution for sampling float params\n",
    "from scipy.stats import randint #C: integer distribution for sampling int params\n",
    "\n",
    "model = XGBRFClassifier(random_state=42) #C: creates the XGBoost RF model/classifier with fixed randomness\n",
    "params = {\n",
    "    \"learning_rate\": uniform(1e-2, 3e-1), #C: samples learing rate between 0.01 and 0.31\n",
    "    \"min_split_loss\": uniform(0, 10), #C: samles min loss reduction needed for a slpit\n",
    "    \"max_depth\": randint(3, 10), #C: samples max tree depth bewteen 3 and 9\n",
    "    \"subsample\": uniform(0, 1), #C: samples row subsampling rate between 0 and 1\n",
    "    \"objective\": [\"reg:squarederror\", \"binary:logistic\", \"reg:logistic\"], #C: tries different learning objectives\n",
    "    \"eval_metric\": [\"aucpr\", \"error\"] #C: tries different eval metrics for validation\n",
    "}\n",
    "\n",
    "model_grid = RandomizedSearchCV(model, param_distributions=params, n_jobs=-1, verbose=3, n_iter=10, cv=10) #C: sets up random hyperparameter search for the model\n",
    "\n",
    "model_grid.fit(X_train, y_train) #C: fit the model grid to training data and find best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score #C: function to calculate accuracy\n",
    "\n",
    "best_model_xgboost_params = model_grid.best_params_ #C: gets the best hyperparameters found\n",
    "print(\"Best xgboost params\") \n",
    "pprint(best_model_xgboost_params) #C: prints the best hyperparameters, remove or use logging\n",
    "\n",
    "y_pred_train = model_grid.predict(X_train) #C: predicts labels for training data\n",
    "y_pred_test = model_grid.predict(X_test) #C: predicts labels for test data\n",
    "print(\"Accuracy train\", accuracy_score(y_pred_train, y_train )) #C: prints training accuracy, remove or use logging\n",
    "print(\"Accuracy test\", accuracy_score(y_pred_test, y_test)) #C: prints test accuracy, remove or use logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost performance overview\n",
    "* Confusion matrix\n",
    "* Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#C: Test set evaluation \n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test) #C: compute confusion matrix for test data\n",
    "print(\"Test actual/predicted\\n\")\n",
    "print(pd.crosstab(y_test, y_pred_test, rownames=['Actual'], colnames=['Predicted'], margins=True),'\\n') #C: prints confusion matrix as table\n",
    "print(\"Classification report\\n\")\n",
    "print(classification_report(y_test, y_pred_test),'\\n') #C: prints classification report for test data\n",
    "#C: ^ above print statements should be remove or logged\n",
    "\n",
    "#C: Train set evaluation\n",
    "conf_matrix = confusion_matrix(y_train, y_pred_train)\n",
    "print(\"Train actual/predicted\\n\")\n",
    "print(pd.crosstab(y_train, y_pred_train, rownames=['Actual'], colnames=['Predicted'], margins=True),'\\n')\n",
    "print(\"Classification report\\n\")\n",
    "print(classification_report(y_train, y_pred_train),'\\n')\n",
    "#C: same as for the test set evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save best XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = model_grid.best_estimator_ #C: get the best trained XGBoost model\n",
    "xgboost_model_path = \"./artifacts/lead_model_xgboost.json\" #C: file path to save the model in artifacts folder\n",
    "xgboost_model.save_model(xgboost_model_path) #C: save the model to a JSON file in artifacts folder\n",
    "#C: not saved to MLflow (?)\n",
    "\n",
    "model_results = {\n",
    "    xgboost_model_path: classification_report(y_train, y_pred_train, output_dict=True) \n",
    "}\n",
    "#C: store train classification metrics for the saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc #C: MLflow PythonModel interface\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os #C: file-system utilities (path handling, directory creation etc.) - check actually used in code, since it's matted out\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score #C: evaluation metrics - check if cohen_kappa_score is used in the code\n",
    "import matplotlib.pyplot as plt #C: plotting library\n",
    "import joblib #C: used to serialize Python objects\n",
    "\n",
    "class lr_wrapper(mlflow.pyfunc.PythonModel): #C: custom wrapper for MLflow to log predict_proba outputs\n",
    "    def __init__(self, model):\n",
    "        self.model = model #C: wrapper stores model internally for use in prediction.\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        return self.model.predict_proba(model_input)[:, 1] #C: probability for each class, extracting only probability of \"class 1\"\n",
    "\n",
    "\n",
    "mlflow.sklearn.autolog(log_input_examples=True, log_models=False) #C: automatically log parameters, metrics, and input examples (log_input_examples=True)\n",
    "experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id #C: get experiment ID\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_id) as run: #C: start MLflow run\n",
    "    model = LogisticRegression() #C: create logistic regression (LR) model\n",
    "    lr_model_path = \"./artifacts/lead_model_lr.pkl\" #C: path to save LR model in artifacts folder\n",
    "\n",
    "    #C: define hyperparameter search space\n",
    "    params = {\n",
    "              'solver': [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"], #C: optimization algorithm\n",
    "              'penalty':  [\"none\", \"l1\", \"l2\", \"elasticnet\"], #C: regularization type\n",
    "              'C' : [100, 10, 1.0, 0.1, 0.01] #C: regularization strength (inverse)\n",
    "    }\n",
    "\n",
    "    #C: random search for best hyperparameters\n",
    "    model_grid = RandomizedSearchCV(model, param_distributions= params, verbose=3, n_iter=10, cv=3)\n",
    "    model_grid.fit(X_train, y_train) #C: fit model grid on training data\n",
    "\n",
    "    best_model = model_grid.best_estimator_ #C: get best LR model\n",
    "\n",
    "    y_pred_train = model_grid.predict(X_train) #C: predict training labels\n",
    "    y_pred_test = model_grid.predict(X_test) #C: predict test labels\n",
    "\n",
    "\n",
    "    # log artifacts - C: to MLflow\n",
    "    mlflow.log_metric('f1_score', f1_score(y_test, y_pred_test))\n",
    "    mlflow.log_artifacts(\"artifacts\", artifact_path=\"model\")\n",
    "    mlflow.log_param(\"data_version\", \"00000\")\n",
    "    \n",
    "    # store model for model interpretability\n",
    "    joblib.dump(value=model, filename=lr_model_path)\n",
    "        \n",
    "    # Custom python model for predicting probability - C: logging to MLflow\n",
    "    mlflow.pyfunc.log_model('model', python_model=lr_wrapper(model))\n",
    "\n",
    "#C: classification report for test data\n",
    "model_classification_report = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "best_model_lr_params = model_grid.best_params_ #C: get best LR hyperparameters\n",
    "\n",
    "print(\"Best lr params\")\n",
    "pprint(best_model_lr_params)\n",
    "#C: ^ remove or log\n",
    "\n",
    "print(\"Accuracy train:\", accuracy_score(y_pred_train, y_train ))\n",
    "print(\"Accuracy test:\", accuracy_score(y_pred_test, y_test))\n",
    "#C: ^ remove print or log\n",
    "\n",
    "#C: confusion matrix and report for test data\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"Test actual/predicted\\n\")\n",
    "print(pd.crosstab(y_test, y_pred_test, rownames=['Actual'], colnames=['Predicted'], margins=True),'\\n')\n",
    "print(\"Classification report\\n\")\n",
    "print(classification_report(y_test, y_pred_test),'\\n')\n",
    "#C: ^ remove print or log\n",
    "\n",
    "#C: confusion matrix and report for train data\n",
    "conf_matrix = confusion_matrix(y_train, y_pred_train)\n",
    "print(\"Train actual/predicted\\n\")\n",
    "print(pd.crosstab(y_train, y_pred_train, rownames=['Actual'], colnames=['Predicted'], margins=True),'\\n')\n",
    "print(\"Classification report\\n\")\n",
    "print(classification_report(y_train, y_pred_train),'\\n')\n",
    "#C: ^ remove print or log\n",
    "\n",
    "model_results[lr_model_path] = model_classification_report #C: store test classification report in results dictionary\n",
    "print(model_classification_report[\"weighted avg\"][\"f1-score\"])\n",
    "#C: ^ remove print or log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save columns and model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list_path = './artifacts/columns_list.json' #C: path to save list of feature columns in articats folder\n",
    "with open(column_list_path, 'w+') as columns_file:\n",
    "    columns = {'column_names': list(X_train.columns)} #C: create dictionary of column names from training data\n",
    "    pprint(columns) #C: remove\n",
    "    json.dump(columns, columns_file) #C: save column names to JSON file\n",
    "\n",
    "print('Saved column list to ', column_list_path) #C: remove or log\n",
    "\n",
    "model_results_path = \"./artifacts/model_results.json\" #C: path to save model results in articats folder\n",
    "with open(model_results_path, 'w+') as results_file:\n",
    "    json.dump(model_results, results_file) #C: save model evaluation metrics to JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL SELECTION\n",
    "\n",
    "Model selection involves choosing the most suitable statistical model from a set of candidates. In straightforward cases, this process uses an existing dataset. When candidate models offer comparable predictive or explanatory power, the simplest model is generally the preferred choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants used:\n",
    "current_date = datetime.datetime.now().strftime(\"%Y_%B_%d\") #C: gets current date and time and formats it as yyyy_monthname_dd\n",
    "artifact_path = \"model\" #C: folder path to save MLflow artifacts\n",
    "model_name = \"lead_model\" #C: name of the model\n",
    "experiment_name = current_date #C: uses the current date as the experiment name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from mlflow.tracking.client import MlflowClient #C: MLflow client to interact with tracking server\n",
    "from mlflow.entities.model_registry.model_version_status import ModelVersionStatus #C: enumeration for model version status\n",
    "from mlflow.tracking.client import MlflowClient #C: repeated line - remove\n",
    "\n",
    "def wait_until_ready(model_name, model_version):\n",
    "    client = MlflowClient() #C: create MLflow client instance\n",
    "    for _ in range(10):\n",
    "        model_version_details = client.get_model_version( #C: fetch details of specific model version\n",
    "          name=model_name, \n",
    "          version=model_version,\n",
    "        )\n",
    "        status = ModelVersionStatus.from_string(model_version_details.status) #C: convert status string to enumeration\n",
    "        print(f\"Model status: {ModelVersionStatus.to_string(status)}\") #C: print current model status, remove or log(?)\n",
    "        if status == ModelVersionStatus.READY: #C: stop waiting if model is ready\n",
    "            break\n",
    "        time.sleep(1) #C: wait 1 second before checking again\n",
    "\n",
    "#C: ^ repeatedly checks the status of a single MLflow model version until it is READY (or up to 10 attempts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting experiment model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_ids = [mlflow.get_experiment_by_name(experiment_name).experiment_id] #C: get the ID of the experiment\n",
    "experiment_ids #C: display, remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_best = mlflow.search_runs(\n",
    "    experiment_ids=experiment_ids, #C: search runs within this experiment\n",
    "    order_by=[\"metrics.f1_score DESC\"], #C: sort runs by F1-score descending\n",
    "    max_results=1 #C: return only the top run\n",
    ").iloc[0] #C: select the first row (best run)\n",
    "experiment_best #C: display - remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./artifacts/model_results.json\", \"r\") as f: #C: open previously saved model results JSON file\n",
    "    model_results = json.load(f)\n",
    "results_df = pd.DataFrame({model: val[\"weighted avg\"] for model, val in model_results.items()}).T #C: extract \"weighted avg\" metrics for each model\n",
    "results_df #C: display - remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = results_df.sort_values(\"f1-score\", ascending=False).iloc[0].name  #C: find the model with highest F1-score\n",
    "print(f\"Best model: {best_model}\") #C: remove or log (?)\n",
    "\n",
    "#C: is this even used elsewhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get production model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient() #C: create MLflow client instance\n",
    "prod_model = [model for model in client.search_model_versions(f\"name='{model_name}'\") if dict(model)['current_stage']=='Production'] #C: search for all registered versions of the model that are in 'Production' stage\n",
    "prod_model_exists = len(prod_model)>0  #C: check if any production model exists\n",
    "\n",
    "if prod_model_exists:\n",
    "    prod_model_version = dict(prod_model[0])['version'] #C: get version number of first production model\n",
    "    prod_model_run_id = dict(prod_model[0])['run_id'] #C: get run ID of first production model\n",
    "    \n",
    "    print('Production model name: ', model_name)\n",
    "    print('Production model version:', prod_model_version)\n",
    "    print('Production model run id:', prod_model_run_id)\n",
    "    #C: ^ remove or log(?)\n",
    "    \n",
    "else:\n",
    "    print('No model in production')\n",
    "    #C: ^ remove or log(?)\n",
    "\n",
    "#C: ^ perhaps make this a helper function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare prod and best trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_score = experiment_best[\"metrics.f1_score\"] #C: F1-score of the best run from current experiment - this is only LR runs (XGBOOSt was not wrapped in MLflow run)\n",
    "model_details = {}\n",
    "model_status = {} #C: placeholder to track current vs production scores\n",
    "run_id = None\n",
    "\n",
    "if prod_model_exists:\n",
    "    data, details = mlflow.get_run(prod_model_run_id) #C: fetch metrics and details of current production model\n",
    "    prod_model_score = data[1][\"metrics.f1_score\"] #C: extract F1-score of production model\n",
    "\n",
    "    model_status[\"current\"] = train_model_score #C: store F1-score of candidate model\n",
    "    model_status[\"prod\"] = prod_model_score #C: store F1-score of production model\n",
    "\n",
    "    if train_model_score>prod_model_score: #C: compare scores - candidate model is better; mark for registration\n",
    "        print(\"Registering new model\") #C: remove or log\n",
    "        run_id = experiment_best[\"run_id\"] #C: store run ID of best candidate model\n",
    "else:\n",
    "    print(\"No model in production\") #C: remove or log\n",
    "    run_id = experiment_best[\"run_id\"] #C: register the best candidate model by default\n",
    "\n",
    "print(f\"Registered model: {run_id}\") #C: remove or log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_id is not None:\n",
    "    print(f'Best model found: {run_id}') #C: remove or log (?)\n",
    "\n",
    "    model_uri = \"runs:/{run_id}/{artifact_path}\".format( #C: create MLflow URI to locate model artifacts\n",
    "        run_id=run_id,\n",
    "        artifact_path=artifact_path\n",
    "    )\n",
    "    model_details = mlflow.register_model(model_uri=model_uri, name=model_name) #C: register the model in MLflow Model Registry\n",
    "    wait_until_ready(model_details.name, model_details.version) #C: wait until the model version is fully available in the registry\n",
    "    model_details = dict(model_details) #C: convert model details to dictionary for easier access/logging\n",
    "    print(model_details) #C: remove print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPLOY\n",
    "\n",
    "A model version can be assigned to one or more stages. MLflow provides predefined stages for common use cases: None, Staging, Production, and Archived. With the necessary permissions, you can transition a model version between stages or request a transition to a different stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = 1 #C: manually specify the version number of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transition to staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "\n",
    "def wait_for_deployment(model_name, model_version, stage='Staging'): #C: waits until the specified model version reaches the desired stage\n",
    "    status = False #C: flag to track deployment status\n",
    "    while not status:\n",
    "        model_version_details = dict( \n",
    "            client.get_model_version(name=model_name,version=model_version) #C: get details of the model version\n",
    "            )\n",
    "        if model_version_details['current_stage'] == stage: #C: check if model reached target stage\n",
    "            print(f'Transition completed to {stage}') #C: remove or log(?)\n",
    "            status = True\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(2) #C: wait 2 seconds before checking again\n",
    "    return status #C: return True when deployment is complete\n",
    "\n",
    "model_version_details = dict(client.get_model_version(name=model_name,version=model_version)) #C: fetch current model version details\n",
    "model_status = True #C: flag to track whether model transition happened successfully\n",
    "if model_version_details['current_stage'] != 'Staging': #C: check if model is already in Staging\n",
    "    client.transition_model_version_stage( #C: move a registered model version to a different stage in the MLflow Model Registry\n",
    "        name=model_name,\n",
    "        version=model_version,stage=\"Staging\", \n",
    "        archive_existing_versions=True #C: archive any other versions currently in Staging\n",
    "    )\n",
    "    model_status = wait_for_deployment(model_name, model_version, 'Staging') #C: wait until model reaches Staging\n",
    "else:\n",
    "    print('Model already in staging') #C: remove or log(?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
